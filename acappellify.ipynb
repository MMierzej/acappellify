{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Each section is comprehensively explained in the Chapter 3: _Methodology_ of the thesis.\n",
        "\n",
        "To run experiments, run the sections for each of them to set up automatically, and then naviage to the _Experiments_ section and run its code.\n",
        "\n",
        "Bear in mind that installing dependencies and downloading models may take 5 minutes or more."
      ],
      "metadata": {
        "id": "uimCq5GmU6eY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpcaGKm6fK2e"
      },
      "source": [
        "# Musical source separation (Demucs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "691C18JIfD-X"
      },
      "outputs": [],
      "source": [
        "!python3 -m pip install -qU git+https://github.com/facebookresearch/demucs#egg=demucs\n",
        "!pip install -q torchvision==0.15.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHVTAgZnfPfJ"
      },
      "outputs": [],
      "source": [
        "# heavily sourced from https://colab.research.google.com/drive/1dC9nVxk3V_VPjUADsnFu8EiT-xnU1tGH?usp=sharing\n",
        "\n",
        "import io\n",
        "from pathlib import Path\n",
        "import select\n",
        "from shutil import rmtree\n",
        "import subprocess as sp\n",
        "import sys\n",
        "from typing import Dict, Tuple, Optional, IO\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "class Demucs:\n",
        "    def __init__(self, model: str = \"htdemucs_ft\") -> None:\n",
        "        self.model = model\n",
        "\n",
        "    def separate(self, inp: Path, outp: Path) -> Path:\n",
        "        cmd = [\"python3\", \"-m\", \"demucs.separate\", \"-o\", str(outp), \"-n\", self.model, str(inp)]\n",
        "        print(\"Executing command: \", \" \".join(cmd))\n",
        "        p = sp.Popen(cmd, stdout=sp.PIPE, stderr=sp.PIPE)\n",
        "        self.__copy_process_streams(p)\n",
        "        p.wait()\n",
        "        if p.returncode != 0:\n",
        "            raise RuntimeError(f\"Command failed: {' '.join(cmd)}\")\n",
        "        return Path(outp) / self.model / inp.stem\n",
        "\n",
        "    def from_upload(self) -> None:\n",
        "        out_path = Path('separated')\n",
        "        in_path = Path('tmp_in')\n",
        "\n",
        "        if in_path.exists():\n",
        "            rmtree(in_path)\n",
        "        in_path.mkdir()\n",
        "\n",
        "        if out_path.exists():\n",
        "            rmtree(out_path)\n",
        "        out_path.mkdir()\n",
        "\n",
        "        uploaded = files.upload()\n",
        "        for name, content in uploaded.items():\n",
        "            (in_path / name).write_bytes(content)\n",
        "        self.separate(in_path, out_path)\n",
        "\n",
        "    def __copy_process_streams(self, process: sp.Popen) -> None:\n",
        "        def raw(stream: Optional[IO[bytes]]) -> IO[bytes]:\n",
        "            assert stream is not None\n",
        "            if isinstance(stream, io.BufferedIOBase):\n",
        "                stream = stream.raw\n",
        "            return stream\n",
        "\n",
        "        p_stdout, p_stderr = raw(process.stdout), raw(process.stderr)\n",
        "        stream_by_fd: Dict[int, Tuple[IO[bytes], io.StringIO, IO[str]]] = {\n",
        "            p_stdout.fileno(): (p_stdout, sys.stdout),\n",
        "            p_stderr.fileno(): (p_stderr, sys.stderr),\n",
        "        }\n",
        "        fds = list(stream_by_fd.keys())\n",
        "\n",
        "        while fds:\n",
        "            # `select` syscall will wait until one of the file descriptors has content.\n",
        "            ready, _, _ = select.select(fds, [], [])\n",
        "            for fd in ready:\n",
        "                p_stream, std = stream_by_fd[fd]\n",
        "                raw_buf = p_stream.read(2 ** 16)\n",
        "                if not raw_buf:\n",
        "                    fds.remove(fd)\n",
        "                    continue\n",
        "                buf = raw_buf.decode()\n",
        "                std.write(buf)\n",
        "                std.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kyt9BXMDhkUq"
      },
      "source": [
        "# Transcription to MIDI (Basic Pitch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tP5JKMJphxgg"
      },
      "outputs": [],
      "source": [
        "!pip install -q librosa pretty_midi basic-pitch[onnx]  # onnx to run on CPU to avoid conflicts with the CUDA libs downgraded by demucs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00Ktrvf5hlst"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from typing import Union\n",
        "\n",
        "import basic_pitch as bp\n",
        "import basic_pitch.inference\n",
        "from basic_pitch.inference import Model as BasicPitchModel\n",
        "import librosa\n",
        "from pretty_midi import PrettyMIDI\n",
        "\n",
        "\n",
        "def _get_bp_model_path(model_type: BasicPitchModel.MODEL_TYPES) -> Path:\n",
        "    filename_suffix = None\n",
        "    match model_type:\n",
        "        case BasicPitchModel.MODEL_TYPES.ONNX:\n",
        "            filename_suffix = bp.FilenameSuffix.onnx\n",
        "        case BasicPitchModel.MODEL_TYPES.TENSORFLOW:\n",
        "            filename_suffix = bp.FilenameSuffix.tf\n",
        "        case BasicPitchModel.MODEL_TYPES.TFLITE:\n",
        "            filename_suffix = bp.FilenameSuffix.tflite\n",
        "        case _:\n",
        "            filename_suffix = bp.FilenameSuffix.onnx\n",
        "    return bp.build_icassp_2022_model_path(filename_suffix)\n",
        "\n",
        "\n",
        "class BasicPitch:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_type: BasicPitchModel.MODEL_TYPES = BasicPitchModel.MODEL_TYPES.ONNX,\n",
        "    ) -> None:\n",
        "        self.model = bp.inference.Model(_get_bp_model_path(model_type))\n",
        "\n",
        "    def get_midi(\n",
        "        self,\n",
        "        audio_path: Union[Path, str],\n",
        "        midi_bpm: float = 120.0,\n",
        "    ) -> PrettyMIDI:\n",
        "        _, midi, _ = bp.inference.predict(audio_path,\n",
        "                                          model_or_model_path=self.model,\n",
        "                                          onset_threshold=0.7,\n",
        "                                          frame_threshold=0.35,\n",
        "                                          midi_tempo=midi_bpm)\n",
        "        return midi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3f4Sf0-1NAA"
      },
      "source": [
        "# MIDI processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tCla_mpJixHj"
      },
      "outputs": [],
      "source": [
        "!pip install -q pretty_midi librosa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMQtXgx61RBA"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from itertools import groupby\n",
        "import re\n",
        "\n",
        "import librosa\n",
        "from pretty_midi import Instrument, Note, PrettyMIDI\n",
        "\n",
        "\n",
        "def midi_from_notes(track: list[Note]) -> PrettyMIDI:\n",
        "    midi = PrettyMIDI()\n",
        "    instrument = Instrument(program=0)\n",
        "    midi.instruments.append(instrument)\n",
        "    instrument.notes = track\n",
        "    return midi\n",
        "\n",
        "def extract_octave(note: Note) -> int:\n",
        "    symbol = librosa.midi_to_note(note.pitch, unicode=False)\n",
        "    octave = re.search(r\"\\d+$\", symbol)\n",
        "    if octave is not None:\n",
        "        return int(octave.group())\n",
        "    else:\n",
        "        raise ValueError(f\"Couldn't extract octave from symbol: {symbol}\")\n",
        "\n",
        "def split_into_octaves(midi: PrettyMIDI) -> dict[int, PrettyMIDI]:\n",
        "    midi_notes = [note for note in midi.instruments[0].notes if note.pitch >= librosa.note_to_midi(\"A0\")]\n",
        "\n",
        "    notes_by_octave = defaultdict(list)\n",
        "    for octave, notes in groupby(midi_notes, extract_octave):\n",
        "        notes_by_octave[octave].extend(notes)\n",
        "\n",
        "    return {octave: midi_from_notes(notes) for octave, notes in notes_by_octave.items()}\n",
        "\n",
        "def transpose_by_semitones(midi: PrettyMIDI, semitones: int) -> PrettyMIDI:\n",
        "    transposed_notes = []\n",
        "    for note in midi.instruments[0].notes:\n",
        "        transposed_note = Note(note.velocity, note.pitch + semitones, note.start, note.end)\n",
        "        transposed_notes.append(transposed_note)\n",
        "    return midi_from_notes(transposed_notes)\n",
        "\n",
        "def to_octave(midi: PrettyMIDI, octave: int) -> PrettyMIDI:\n",
        "    def adjust_octave(note: Note) -> Note:\n",
        "        current_octave = extract_octave(note)\n",
        "        semitones_diff = 12 * (octave - current_octave)\n",
        "        return Note(note.velocity, note.pitch + semitones_diff, note.start, note.end)\n",
        "\n",
        "    return midi_from_notes(list(map(adjust_octave, midi.instruments[0].notes)))\n",
        "\n",
        "def constrain_pitch_range(midi: PrettyMIDI, min_octave: int, max_octave: int) -> PrettyMIDI:\n",
        "    filtered_notes = []\n",
        "    for note in midi.instruments[0].notes:\n",
        "        if min_octave <= extract_octave(note) <= max_octave:\n",
        "            filtered_notes.append(note)\n",
        "    return midi_from_notes(filtered_notes)\n",
        "\n",
        "def to_many_monophonic(poly_midi: PrettyMIDI) -> list[PrettyMIDI]:\n",
        "    mono_tracks = []\n",
        "\n",
        "    for note in sorted(poly_midi.instruments[0].notes, key=lambda note: note.start):\n",
        "        track = next((track for track in mono_tracks if track[-1].end <= note.start), [])\n",
        "        if len(track) == 0:\n",
        "            mono_tracks.append(track)\n",
        "        track.append(note)\n",
        "\n",
        "    return list(map(midi_from_notes, mono_tracks))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Kb0WHUCiIZ8"
      },
      "source": [
        "# Vocal synthesis (DiffSinger)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "f0gnnEUyv1zB"
      },
      "outputs": [],
      "source": [
        "!git clone --quiet --single-branch https://github.com/MMierzej/diffsinger.git\n",
        "!pip install -q -r diffsinger/requirements-colab-inference-e2e.txt\n",
        "!pip install -q pretty_midi pydub\n",
        "!cp -r diffsinger/configs .  # workaround\n",
        "!mkdir -p usr\n",
        "!cp -r diffsinger/usr/configs usr/  # workaround\n",
        "!mkdir checkpoints\n",
        "\n",
        "# pretrained DiffSinger model\n",
        "!curl -L -O \\\n",
        "  -H \"Accept: application/vnd.github+json\" \\\n",
        "  -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n",
        "  https://github.com/MoonInTheRiver/DiffSinger/releases/download/pretrain-model/0228_opencpop_ds100_rel.zip\n",
        "!unzip -q 0228_opencpop_ds100_rel.zip model_ckpt_steps_160000.ckpt config.yaml -d checkpoints/0228_opencpop_ds100_rel\n",
        "\n",
        "# pitch estimator\n",
        "!curl -L -O \\\n",
        "  -H \"Accept: application/vnd.github+json\" \\\n",
        "  -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n",
        "  https://github.com/MoonInTheRiver/DiffSinger/releases/download/pretrain-model/0102_xiaoma_pe.zip\n",
        "!unzip -q 0102_xiaoma_pe.zip -d checkpoints/0102_xiaoma_pe\n",
        "\n",
        "# vocoder (mel-spectrogram -> waveform)\n",
        "#   config\n",
        "!curl -L -O \\\n",
        "  -H \"Accept: application/vnd.github+json\" \\\n",
        "  -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n",
        "  https://github.com/MoonInTheRiver/DiffSinger/releases/download/pretrain-model/0109_hifigan_bigpopcs_hop128.zip\n",
        "!unzip -q 0109_hifigan_bigpopcs_hop128.zip config.yaml -d checkpoints/0109_hifigan_bigpopcs_hop128\n",
        "#   model\n",
        "!curl -L --output checkpoints/0109_hifigan_bigpopcs_hop128/model_ckpt_steps_1512000.ckpt \\\n",
        "  -H \"Accept: application/vnd.github+json\" \\\n",
        "  -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n",
        "  https://github.com/MoonInTheRiver/DiffSinger/releases/download/pretrain-model/model_ckpt_steps_1512000.ckpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SIpWZyavoyC"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import math\n",
        "import re\n",
        "import tempfile\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "from pretty_midi import PrettyMIDI\n",
        "from pydub import AudioSegment\n",
        "\n",
        "from diffsinger.inference.svs.ds_e2e import DiffSingerE2EInfer\n",
        "from diffsinger.utils.audio import save_wav\n",
        "from diffsinger.utils.hparams import set_hparams\n",
        "\n",
        "\n",
        "class DiffSinger:\n",
        "    SILENCE_MIN_DURATION_S = 0.4\n",
        "    _MIDI_PITCH_BREATH = -1\n",
        "    _MIDI_PITCH_SILENCE = -2\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        config_path: str = \"usr/configs/midi/e2e/opencpop/ds100_adj_rel.yaml\",\n",
        "        experiment_name: str = \"0228_opencpop_ds100_rel\",\n",
        "    ):\n",
        "        hparams = set_hparams(\n",
        "            config=config_path,\n",
        "            exp_name=experiment_name,\n",
        "            print_hparams=False,\n",
        "        )\n",
        "        self.model = DiffSingerE2EInfer(hparams)\n",
        "        self.sample_rate = hparams[\"audio_sample_rate\"]\n",
        "\n",
        "    def vocalize(self, mono_midi: PrettyMIDI) -> AudioSegment:\n",
        "        ds_batches_and_offsets = self._mono_midi_to_ds_batches(mono_midi)\n",
        "        ds_batches = [ds_batch for ds_batch, _ in ds_batches_and_offsets]\n",
        "        offsets = [offset for _, offset in ds_batches_and_offsets]\n",
        "\n",
        "        wavs = [self.model.infer_once(ds_batch) for ds_batch in ds_batches]\n",
        "\n",
        "        vocal_segments = []\n",
        "        for wav in wavs:\n",
        "            with tempfile.NamedTemporaryFile() as tmp_f:\n",
        "                save_wav(wav, tmp_f.name, self.sample_rate)\n",
        "                vocal_segment = AudioSegment.from_wav(tmp_f.name)\n",
        "                vocal_segments.append(vocal_segment)\n",
        "\n",
        "        midi_end_s = mono_midi.instruments[0].notes[-1].end\n",
        "        durations_ms = [round((next - current) * 1000)\n",
        "                        for current, next in zip([0.0] + offsets,\n",
        "                                                 offsets + [midi_end_s],\n",
        "                                                 strict=True)]\n",
        "\n",
        "        vocal = AudioSegment.silent(durations_ms[0], self.sample_rate)\n",
        "        for vocal_segment, duration_ms in zip(vocal_segments, durations_ms[1:], strict=True):\n",
        "            adjusted_vocal_segment = None\n",
        "            if len(vocal_segment) > duration_ms:\n",
        "                adjusted_vocal_segment = vocal_segment.fade(\n",
        "                    to_gain=-120.0, end=duration_ms, duration=self.SILENCE_MIN_DURATION_S  # every segment's input should end with a silence\n",
        "                )[:duration_ms]\n",
        "            else:\n",
        "                padding = AudioSegment.silent(duration_ms - len(vocal_segment), self.sample_rate)\n",
        "                adjusted_vocal_segment = vocal_segment + padding\n",
        "            vocal += adjusted_vocal_segment\n",
        "\n",
        "        return vocal\n",
        "\n",
        "    def _mono_midi_to_ds_batches(\n",
        "        self,\n",
        "        mono_midi: PrettyMIDI,\n",
        "        single_octave: Optional[int] = None,\n",
        "    ) -> list[tuple[dict[str, str], float]]:\n",
        "        # TODO: cap batch length too\n",
        "\n",
        "        ds_notes = self._mono_midi_to_ds_notes(mono_midi, single_octave)\n",
        "\n",
        "        batches = []\n",
        "        offset = 0.0\n",
        "        current_batch = []\n",
        "        current_batch_offset = 0.0\n",
        "        for symbol, duration, phonemes in ds_notes + [(\"rest\", self.SILENCE_MIN_DURATION_S, [\"SP\"])]:\n",
        "            offset += duration\n",
        "            current_batch.append((symbol, duration, phonemes))\n",
        "            if phonemes == [\"SP\"]:\n",
        "                batches.append((current_batch, current_batch_offset))\n",
        "                current_batch = []\n",
        "                current_batch_offset = offset\n",
        "\n",
        "        return [(self._ds_notes_to_dict(batch), offset) for batch, offset in batches]\n",
        "\n",
        "    def _mono_midi_to_ds_notes(\n",
        "        self,\n",
        "        mono_midi: PrettyMIDI,\n",
        "        single_octave: Optional[int] = None,\n",
        "    ) -> list[tuple[str, float, list[str]]]:\n",
        "        def midi_pitch_to_note_symbol(pitch: int) -> str:\n",
        "            symbol = librosa.midi_to_note(pitch, unicode=False) if pitch >= 0 else \"rest\"\n",
        "            if single_octave is not None:\n",
        "                symbol = re.sub(r\"\\d+\", str(single_octave), symbol)\n",
        "            return symbol\n",
        "\n",
        "        def midi_pitch_to_phonemes(pitch: int) -> list[str]:\n",
        "            match pitch:\n",
        "                case pitch if pitch >= 0:\n",
        "                    return [\"n\", \"a\"]\n",
        "                case self._MIDI_PITCH_BREATH:\n",
        "                    return [\"AP\"]\n",
        "                case self._MIDI_PITCH_SILENCE:\n",
        "                    return [\"SP\"]\n",
        "                case _:\n",
        "                    raise ValueError(f\"Unknown pitch: {pitch}\")\n",
        "\n",
        "        def construct_note(pitch: int, duration: float) -> tuple[str, float, list[str]]:\n",
        "            symbol = midi_pitch_to_note_symbol(pitch)\n",
        "            phonemes = midi_pitch_to_phonemes(pitch)\n",
        "            return (symbol, duration, phonemes)\n",
        "\n",
        "        notes = []  # list[tuple[symbol, duration, list[phoneme]]]\n",
        "        midi_notes = mono_midi.instruments[0].notes\n",
        "        midi_start_offset = midi_notes[0].start\n",
        "        if midi_start_offset != 0:\n",
        "            # insert silence at the beginning\n",
        "            notes.append(construct_note(self._MIDI_PITCH_SILENCE, midi_start_offset))\n",
        "        for current_note, next_note in zip(midi_notes, midi_notes[1:]):\n",
        "            gap = next_note.start - current_note.end\n",
        "            if gap < self.SILENCE_MIN_DURATION_S:\n",
        "                # lengthen current note\n",
        "                notes.append(construct_note(current_note.pitch, next_note.start - current_note.start))\n",
        "            else:\n",
        "                # split into note and rest\n",
        "                notes.append(construct_note(current_note.pitch, current_note.end - current_note.start))\n",
        "                notes.append(construct_note(self._MIDI_PITCH_SILENCE, gap))\n",
        "        last_midi_note = midi_notes[-1]\n",
        "        notes.append(construct_note(last_midi_note.pitch, last_midi_note.end - last_midi_note.start))\n",
        "\n",
        "        return notes\n",
        "\n",
        "    def _ds_notes_to_dict(self, ds_notes: list[tuple[str, float, list[str]]]) -> dict[str, str]:\n",
        "        notes = [(symbol, duration, phoneme) for symbol, duration, phonemes in ds_notes for phoneme in phonemes]\n",
        "        return {\n",
        "            \"input_type\": \"phoneme\",\n",
        "            \"text\": \"\",  # doesn't matter, but is required\n",
        "            \"ph_seq\": \" \".join(phoneme for _, _, phoneme in notes),\n",
        "            \"note_seq\": \" \".join(symbol for symbol, _, _ in notes),\n",
        "            \"note_dur_seq\": \" \".join(str(duration) for _, duration, _ in notes),\n",
        "            \"is_slur_seq\": \" \".join(\"0\" * len(notes)),\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Singing voice conversion (Fish Diffusion)"
      ],
      "metadata": {
        "id": "srITamJhGXWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/fishaudio/fish-diffusion.git fishdiffusion\n",
        "%cd fishdiffusion\n",
        "!git checkout 8b21f57080e70675aaaa2ffa2fad04aed9119420\n",
        "!sed -i 's/from fish_audio_preprocess.utils import loudness_norm, separate_audio/from fish_audio_preprocess.utils import loudness_norm/' fish_diffusion/utils/audio.py\n",
        "%cd ..\n",
        "!cp -r fishdiffusion/configs/_base_/ configs/\n",
        "!mkdir -p configs/\n",
        "!mkdir -p checkpoints/\n",
        "!curl -s -L --output configs/M4Singer.py https://huggingface.co/spaces/fishaudio/fish-diffusion/resolve/main/configs/M4Singer.py?download=true\n",
        "!curl -s -L --output checkpoints/M4Singer.ckpt https://huggingface.co/spaces/fishaudio/fish-diffusion/resolve/main/checkpoints/M4Singer.ckpt?download=true\n",
        "\n",
        "!pip install -q mmengine==0.4.0 fish-audio-preprocess==0.2.8 torchcrepe pyworld"
      ],
      "metadata": {
        "collapsed": true,
        "id": "oscAuLs6GZS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mmengine import Config\n",
        "\n",
        "%cd fishdiffusion\n",
        "from tools.hifisinger.inference import HiFiSingerSVCInference\n",
        "%cd ..\n",
        "\n",
        "\n",
        "def get_speaker_for_octave(octave: int) -> str:\n",
        "    if octave >= 5:\n",
        "        return \"M4Singer-Soprano-1\"\n",
        "    elif octave >= 4:\n",
        "        return \"M4Singer-Alto-1\"\n",
        "    elif octave >= 3:\n",
        "        return \"M4Singer-Tenor-1\"\n",
        "    else:\n",
        "        return \"M4Singer-Bass-1\""
      ],
      "metadata": {
        "id": "2bw7xvZ8Jyi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJwVeHTiJeba"
      },
      "source": [
        "# Mixing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z30YRhY-JgOo"
      },
      "outputs": [],
      "source": [
        "!sudo apt -qq install -y ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from typing import Union\n",
        "\n",
        "\n",
        "# TODO: extract the Acappellifier::_ffmpeg_mix method into here?\n",
        "\n",
        "\n",
        "def normalize_audio(\n",
        "    audio_path: Union[str, Path],\n",
        "    output_path: Union[str, Path],\n",
        "    lufs: int = -14,\n",
        "    lra: int = 7,\n",
        "    peak_db: int = -1,\n",
        ") -> None:\n",
        "    ffmpeg_norm_cmd = [\n",
        "        \"ffmpeg -y\",\n",
        "        f\"-i {audio_path}\",\n",
        "        \"-filter:a\",\n",
        "        f\"\\\"loudnorm=I={lufs}:LRA={lra}:TP={peak_db}\\\"\",\n",
        "        str(output_path),\n",
        "    ]\n",
        "    subprocess.run(\" \".join(ffmpeg_norm_cmd), shell=True, check=True)"
      ],
      "metadata": {
        "id": "yxRoz3542gHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b98hfYmJkxT"
      },
      "source": [
        "# Acappellifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pydub pretty_midi"
      ],
      "metadata": {
        "id": "OIrhqA9Vsae5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMxCj2vtJoVv"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from itertools import chain\n",
        "from pathlib import Path\n",
        "import pprint\n",
        "import subprocess\n",
        "from tempfile import NamedTemporaryFile\n",
        "from typing import Union\n",
        "\n",
        "from pretty_midi import PrettyMIDI\n",
        "from pydub import AudioSegment\n",
        "\n",
        "\n",
        "DIFFSINGER_FRIENDLY_OCTAVE = 4\n",
        "\n",
        "\n",
        "class Acappellifier:\n",
        "    def __init__(\n",
        "        self,\n",
        "        demucs: Demucs,\n",
        "        basic_pitch: BasicPitch,\n",
        "        diff_singer: DiffSinger,\n",
        "        hifi_singer_svc: HiFiSingerSVCInference,\n",
        "    ) -> None:\n",
        "        self.demucs = demucs\n",
        "        self.basic_pitch = basic_pitch\n",
        "        self.diff_singer = diff_singer\n",
        "        self.hifi_singer_svc = hifi_singer_svc\n",
        "\n",
        "    def acappellify(self, song_path: Union[str, Path]) -> Path:\n",
        "        song_path = Path(song_path)\n",
        "\n",
        "        acappella_segment_paths = []\n",
        "        for segment in self._slice_input(AudioSegment.from_file(song_path)):\n",
        "            with NamedTemporaryFile(delete=False, suffix=\".wav\") as tmpf:\n",
        "                segment.export(tmpf.name, format=\"wav\")\n",
        "                acappella_segment_path = self._acappellify_single(Path(tmpf.name))\n",
        "                acappella_segment_paths.append(acappella_segment_path)\n",
        "\n",
        "        if len(acappella_segment_paths) == 0:\n",
        "            raise RuntimeError(f\"No acappella segments produced for '{song_path}'\")\n",
        "\n",
        "        print(f\"Acapella segment paths: {acappella_segment_paths}\")\n",
        "\n",
        "        # concatenation of output fragments\n",
        "        acappella_segments = list(map(AudioSegment.from_file, acappella_segment_paths))\n",
        "        acappella = acappella_segments[0]\n",
        "        for segment in acappella_segments[1:]:\n",
        "            acappella = acappella.append(segment, crossfade=1000)\n",
        "\n",
        "        acappella_path = Path(\"acappellas\") / f\"{song_path.stem}_acappella.wav\"\n",
        "        acappella_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        acappella.export(acappella_path, format=\"wav\")\n",
        "        return acappella_path\n",
        "\n",
        "    def _acappellify_single(self, song_path: Path) -> Path:\n",
        "        stems_dir = self._separate(Path(song_path), Path(\"separated\"))\n",
        "        stems = [\"other\", \"bass\"]\n",
        "\n",
        "        midi_by_stem = self._get_midi_for_stems(stems, stems_dir)\n",
        "        midi_by_octave_by_stem = {stem: split_into_octaves(midi) for stem, midi in midi_by_stem.items()}\n",
        "        mono_midis_by_octave_by_stem = {stem: {octave: to_many_monophonic(midi) for octave, midi in midi_by_octave.items()}\n",
        "                                        for stem, midi_by_octave in midi_by_octave_by_stem.items()}\n",
        "\n",
        "        vocal_paths_by_octave = self._vocalize_midis(mono_midis_by_octave_by_stem)\n",
        "\n",
        "        song_vocals_path = stems_dir / \"vocals.wav\"\n",
        "        return self._mix(song_vocals_path, vocal_paths_by_octave)\n",
        "\n",
        "    def _vocalize_midis(\n",
        "        self,\n",
        "        mono_midis_by_octave_by_stem: dict[str, dict[int, list[PrettyMIDI]]]\n",
        "    ) -> dict[int, list[Path]]:\n",
        "        output_dir = Path(\"diffsinger_output\") / datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "\n",
        "        vocal_paths_by_octave = defaultdict(list)\n",
        "        for stem, mono_midis_by_octave in mono_midis_by_octave_by_stem.items():\n",
        "            for octave, mono_midis in mono_midis_by_octave.items():\n",
        "                for i, mono_midi in enumerate(mono_midis):\n",
        "                    vocal_path = self._vocalize_mono_midi(mono_midi, octave, i, output_dir / stem)\n",
        "                    vocal_paths_by_octave[octave].append(vocal_path)\n",
        "\n",
        "        return vocal_paths_by_octave\n",
        "\n",
        "    def _vocalize_mono_midi(self, mono_midi: PrettyMIDI, octave: int, i: int, output_dir: Path) -> Path:\n",
        "        semitones_diff = 12 * (DIFFSINGER_FRIENDLY_OCTAVE - octave)\n",
        "        transposed_midi = transpose_by_semitones(mono_midi, semitones_diff)\n",
        "\n",
        "        vocal_segment = self.diff_singer.vocalize(transposed_midi)\n",
        "\n",
        "        output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        vocal_path = output_dir / f\"octave{octave}_mono{i}.wav\"\n",
        "        vocal_segment.export(vocal_path, format=\"wav\")\n",
        "\n",
        "        transposed_vocal_path = self._transpose_vocal(vocal_path, DIFFSINGER_FRIENDLY_OCTAVE, octave)\n",
        "        return transposed_vocal_path\n",
        "\n",
        "    def _transpose_vocal(self, vocal_path: Path, current_octave: int, target_octave: int) -> Path:\n",
        "        semitones_diff = 12 * (target_octave - current_octave)\n",
        "\n",
        "        stem_suffix = f\"transposed{'+' if semitones_diff >= 0 else '-'}{abs(semitones_diff)}\"\n",
        "        transposed_vocal_path = vocal_path.parent / f\"{vocal_path.stem}_{stem_suffix}.wav\"\n",
        "\n",
        "        try:\n",
        "            self.hifi_singer_svc.inference(\n",
        "                input_path=str(vocal_path),\n",
        "                output_path=str(transposed_vocal_path),\n",
        "                speaker=get_speaker_for_octave(target_octave),\n",
        "                pitch_adjust=semitones_diff,\n",
        "                extract_vocals=False,\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            print(f\"Returning unmodified vocal '{vocal_path}'\")\n",
        "            return vocal_path\n",
        "        else:\n",
        "            return transposed_vocal_path\n",
        "\n",
        "    def _separate(self, song_path: Path, output_dir: Path) -> Path:\n",
        "        stems_dir = output_dir / self.demucs.model / song_path.stem\n",
        "        if not stems_dir.exists():\n",
        "            return self.demucs.separate(song_path, output_dir)\n",
        "        return stems_dir\n",
        "\n",
        "    def _get_midi_for_stems(self, stems: list[str], stems_dir: Path) -> dict[str, PrettyMIDI]:\n",
        "        stem_midis = {}\n",
        "        for stem in stems:\n",
        "            stem_path = (stems_dir / stem).with_suffix(\".wav\")\n",
        "            stem_midis[stem] = self._get_midi_for_stem(stem, stem_path)\n",
        "        return stem_midis\n",
        "\n",
        "    def _get_midi_for_stem(self, stem: str, stem_path: Path) -> PrettyMIDI:\n",
        "        min_octave, max_octave = 1, 6\n",
        "        match stem:\n",
        "            case \"other\":\n",
        "                min_octave, max_octave = 3, 6\n",
        "            case \"bass\":\n",
        "                min_octave, max_octave = 1, 2\n",
        "        midi = self.basic_pitch.get_midi(stem_path)\n",
        "        return constrain_pitch_range(midi, min_octave, max_octave)\n",
        "\n",
        "    def _mix(\n",
        "        self,\n",
        "        song_vocals_path: Path,\n",
        "        vocal_paths_by_octave: dict[int, list[Path]]\n",
        "    ) -> Path:\n",
        "        def get_volume_adjustment_db(octave: int) -> int:\n",
        "            if octave >= 3:\n",
        "                return -3\n",
        "            elif octave >= 2:\n",
        "                return -6\n",
        "            else:\n",
        "                return -9\n",
        "\n",
        "        output_dir = Path(\"mixes\") / datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "        output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        output_path = output_dir / \"mix.wav\"\n",
        "\n",
        "        # song_vocals_norm_path = output_dir / \"vocals_norm.wav\"\n",
        "        # normalize_audio(song_vocals_path, song_vocals_norm_path)\n",
        "\n",
        "        audio_paths_and_input_adjustments_db = [\n",
        "            (path, get_volume_adjustment_db(octave))\n",
        "            for octave, paths in vocal_paths_by_octave.items()\n",
        "            for path in paths\n",
        "        ] + [(song_vocals_path, 0)]  # TODO: adjusting the volume reduction\n",
        "\n",
        "        self._ffmpeg_mix(audio_paths_and_input_adjustments_db, output_path)\n",
        "        return output_path\n",
        "\n",
        "    def _ffmpeg_mix(\n",
        "        self,\n",
        "        audio_paths_and_input_adjustments_db: list[tuple[Path, int]],\n",
        "        output_path: Path,\n",
        "    ) -> None:\n",
        "        audio_paths = [path for path, _ in audio_paths_and_input_adjustments_db]\n",
        "        adjustments_db = [reduction for _, reduction in audio_paths_and_input_adjustments_db]\n",
        "\n",
        "        input_args = \" \".join(f\"-i {path.absolute()}\" for path in audio_paths)\n",
        "        volume_filters = \";\".join(f\"[{i}:a]volume={reduction}dB[a{i}]\" for i, reduction in enumerate(adjustments_db))\n",
        "        amix_inputs = \"\".join(f\"[a{i}]\" for i in range(len(audio_paths)))\n",
        "\n",
        "        filter_complex = f\"\\\"{volume_filters};{amix_inputs}amix=inputs={len(audio_paths)}:duration=longest:dropout_transition=2\\\"\"\n",
        "\n",
        "        with NamedTemporaryFile(delete=False, suffix=\".wav\") as tmpf:\n",
        "            ffmpeg_mix_cmd = [\n",
        "                \"ffmpeg -y\",\n",
        "                input_args,\n",
        "                \"-filter_complex\",\n",
        "                filter_complex,\n",
        "                \"-ac 2\",\n",
        "                \"-ar 44100\",\n",
        "                \"-f wav\",\n",
        "                tmpf.name,\n",
        "            ]\n",
        "            subprocess.run(\" \".join(ffmpeg_mix_cmd), shell=True, check=True)\n",
        "            normalize_audio(tmpf.name, output_path)\n",
        "\n",
        "    def _slice_input(self, audio: AudioSegment) -> list[AudioSegment]:\n",
        "        def get_segment_end(\n",
        "            potential_end_ms: int,\n",
        "            total_length_ms: int,\n",
        "            last_segment_min_length_ms: int\n",
        "        ) -> int:\n",
        "            end = min(potential_end_ms, total_length_ms)\n",
        "            if total_length_ms - end < last_segment_min_length_ms:\n",
        "                end = total_length_ms\n",
        "            return end\n",
        "\n",
        "        total_length_ms = len(audio)\n",
        "        first_segment_length_ms = int(10.5 * 1000)\n",
        "        subsequent_segment_length_ms = 11 * 1000\n",
        "        last_segment_min_length_ms = 4 * 1000\n",
        "        overlap_ms = 1000\n",
        "\n",
        "        segments = []\n",
        "\n",
        "        end = get_segment_end(first_segment_length_ms, total_length_ms, last_segment_min_length_ms)\n",
        "        segments.append(audio[:end])\n",
        "        last_end = end\n",
        "        while last_end < total_length_ms:\n",
        "            start = max(0, last_end - overlap_ms)\n",
        "            end = get_segment_end(start + subsequent_segment_length_ms, total_length_ms, last_segment_min_length_ms)\n",
        "            segments.append(audio[start:end])\n",
        "            last_end = end\n",
        "\n",
        "        return segments"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments"
      ],
      "metadata": {
        "id": "qeDoD_ptrGm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pydub"
      ],
      "metadata": {
        "id": "4dg8IrVI7XUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import subprocess\n",
        "from typing import Union\n",
        "\n",
        "from google.colab import files\n",
        "from pydub import AudioSegment\n",
        "\n",
        "\n",
        "def upload_file() -> Union[Path, None]:\n",
        "    uploaded = files.upload()\n",
        "    if uploaded:\n",
        "        return list(uploaded.keys())[0]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def crop_audio(audio_path: Union[str, Path], *, start_time_s: int, duration_s: int) -> Path:\n",
        "    audio_path = Path(audio_path)\n",
        "    if not audio_path.exists():\n",
        "        raise ValueError(f\"The given file '{audio_path}' doesn't exist\")\n",
        "\n",
        "    output_path = audio_path.parent / f\"{audio_path.stem}_ss{start_time_s}_t{duration_s}{audio_path.suffix}\"\n",
        "\n",
        "    ffmpeg_cmd = [\n",
        "        \"ffmpeg -y\",\n",
        "        \"-i\", str(audio_path),\n",
        "        \"-ss\", str(start_time_s),\n",
        "        \"-t\", str(duration_s),\n",
        "        \"-c\", \"copy\",\n",
        "        str(output_path)\n",
        "    ]\n",
        "    subprocess.run(\" \".join(ffmpeg_cmd), shell=True, check=True)\n",
        "\n",
        "    return output_path"
      ],
      "metadata": {
        "id": "iXK7DdFddyXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "demucs = Demucs()\n",
        "basic_pitch = BasicPitch()\n",
        "diff_singer = DiffSinger()\n",
        "hifi_singer_svc = HiFiSingerSVCInference(\n",
        "    Config.fromfile(\"configs/M4Singer.py\"),\n",
        "    \"checkpoints/M4Singer.ckpt\"\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "Lw1LBusUpTbv",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acappellifier = Acappellifier(demucs, basic_pitch, diff_singer, hifi_singer_svc)"
      ],
      "metadata": {
        "id": "hbURR9etBL4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "song_path = upload_file()  # or just a path if the file already exists"
      ],
      "metadata": {
        "id": "S22LylyreoZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert song_path is not None, \"Please, upload a song first\""
      ],
      "metadata": {
        "id": "Xp3Lpukr8KY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: optionally crop the audio so that the processing doesn't take too long\n",
        "song_path = crop_audio(song_path, start_time_s=15, duration_s=10)"
      ],
      "metadata": {
        "id": "zIboZTHn8Csy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# original song\n",
        "AudioSegment.from_file(song_path)"
      ],
      "metadata": {
        "id": "BlQ86EivXSmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "arrangement_path = acappellifier.acappellify(song_path)  # NOTE: first run always takes longer due to lazy imports taking place and models being downloaded\n",
        "AudioSegment.from_file(arrangement_path)"
      ],
      "metadata": {
        "id": "6HxYQ8Y0rJik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(arrangement_path)"
      ],
      "metadata": {
        "id": "8EGQCIsrTs_z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "OpcaGKm6fK2e",
        "Kyt9BXMDhkUq",
        "i3f4Sf0-1NAA",
        "1Kb0WHUCiIZ8",
        "srITamJhGXWi",
        "HJwVeHTiJeba",
        "8b98hfYmJkxT"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}